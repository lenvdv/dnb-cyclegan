{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liquid and Dancefloor Drum and Bass Style Transfer Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the pretrained style transfer system for Drum & Bass music from [our paper](https://biblio.ugent.be/publication/8619952).  \n",
    "This notebook was written by Len Vande Veire.\n",
    "\n",
    "In order to use it, copy some Drum & Bass tracks into the `./_music` directory, execute the following cells, and select your song from the list in the GUI that will appear below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import IPython\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "import autodj\n",
    "import autodj.dj.annotators.wrappers as annot \n",
    "from autodj.dj.song import Song \n",
    "from autodj.dj.timestretching import *\n",
    "from data import CreateDataLoader\n",
    "from generate_util import *\n",
    "from models import create_model\n",
    "from options.test_options import TestOptions\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: extract a fragment from the input audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Drum & Bass tracks you provide in the `./_music` directory will be analyzed and segmented at the (estimated) position of the drop. In the style transfer application, only the extracted fragments of the selected song will be transformed into the other genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_segments_from_song(filename, phrases_before_drop = 2, length_in_phrases = 2):\n",
    "    \n",
    "    annotation_modules = [\n",
    "        annot.BeatAnnotationWrapper(),\n",
    "        annot.OnsetCurveAnnotationWrapper(),\n",
    "        annot.DownbeatAnnotationWrapper(),\n",
    "        annot.StructuralSegmentationWrapper(),\n",
    "        annot.ReplayGainWrapper(),\n",
    "    ]\n",
    "    song = Song(filename, annotation_modules=annotation_modules)\n",
    "    if not song.hasAllAnnot():\n",
    "        print('Annotating song...')\n",
    "        song.annotate()\n",
    "    else:\n",
    "        print('Song already annotated!')\n",
    "\n",
    "    song.open()\n",
    "    song.openAudio()\n",
    "    segments_H = [i for i in range(len(song.segment_types)) if song.segment_types[i] == 'H']\n",
    "\n",
    "    extracted_segments = []\n",
    "    seg_idx = segments_H[0]\n",
    "    start = phrases_before_drop\n",
    "    for i in range(length_in_phrases):\n",
    "        start_idx = int(song.downbeats[song.segment_indices[seg_idx] + (i-start)*4] * 44100)\n",
    "        end_idx = int(song.downbeats[song.segment_indices[seg_idx] + (i-start+1)*4] * 44100)\n",
    "        extracted_segments.append(song.audio[start_idx:end_idx])\n",
    "        \n",
    "    return extracted_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: convert the audio fragment to a spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below transforms each extract into the spectrogram representation that will be transformed by the CycleGAN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_extract(widgets):\n",
    "    return librosa.load(os.path.join('./_music', widgets['file'].value), sr=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_extract_to_png(y, sr, widgets):\n",
    "    \n",
    "    if not os.path.exists('./_temp/'):\n",
    "        os.mkdir('./_temp/')\n",
    "    \n",
    "    for f in glob.glob(\"./_temp/*\"):\n",
    "        os.remove(f)\n",
    "\n",
    "    S_mel, S_stft = mel_spectrogram(y, sr, crop_to_multiple_of_4=True)\n",
    "    S_mel = (S_mel - S_mel.min()) / (S_mel.max() - S_mel.min())\n",
    "    S_mel_as_uint8 = (S_mel * 255).astype(np.uint8)\n",
    "    im = Image.fromarray(S_mel_as_uint8).convert(\"L\")\n",
    "    im.save(os.path.join('./_temp', os.path.splitext(widgets['file'].value)[0] + '.png'))\n",
    "    return S_mel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: apply the CycleGAN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code loads the CycleGAN model and applies it to the extracted audio segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model():\n",
    "    \n",
    "    argv = [\n",
    "        '--dataroot', './_temp',\n",
    "        '--name', 'maps_cyclegan', '--model', 'cycle_gan',\n",
    "        '--dataset_mode', 'single',\n",
    "        '--resize_or_crop', 'none',\n",
    "        '--gpu_ids', '-1',\n",
    "    ]\n",
    "\n",
    "    opt = TestOptions().parse(argv)\n",
    "    opt.num_threads = 1   # test code only supports num_threads = 1\n",
    "    opt.batch_size = 1    # test code only supports batch_size = 1\n",
    "    opt.serial_batches = True  # no shuffle\n",
    "    opt.no_flip = True    # no flip\n",
    "    opt.display_id = -1 # no visdom display\n",
    "\n",
    "    model = create_model(opt)\n",
    "    model.setup(opt)\n",
    "\n",
    "    data_loader = CreateDataLoader(opt)\n",
    "    dataset = data_loader.load_data()\n",
    "    _, data = next(enumerate(dataset))\n",
    "\n",
    "    model.set_input(data)\n",
    "    model.test()\n",
    "    return model.get_current_visuals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code gets the required audio streams from the transformed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(y_orig, img_orig, visuals, widgets):\n",
    "    A_or_B = 'A' if widgets['genre'].value == w_genre_options['A'] else 'B'\n",
    "    y_lws, S_lws, y_no_lws, S_orig, y_lws_orig, img = tensor_to_spectrogram_and_audio(visuals['fake_{}'.format(A_or_B)], y_orig)\n",
    "    \n",
    "    if widgets['show_intermediate'].value:\n",
    "        plt.figure()\n",
    "        plt.imshow(img)    \n",
    "        plt.figure()\n",
    "        plt.imshow(img_orig)\n",
    "        plt.show()\n",
    "        \n",
    "        def _display_audio(y, title):  \n",
    "            a = IPython.display.Audio(y_orig, rate=44100,)\n",
    "            print(title)\n",
    "            display(a)\n",
    "        \n",
    "        _display_audio(y_lws, 'LWS-reconstructed phase')\n",
    "        _display_audio(y_no_lws, 'Original phase')\n",
    "    \n",
    "    return y_lws, y_no_lws, y_lws_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The actual application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a GUI with which you can easily interact with the demo. Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file_options = [f for f in os.listdir('./_music') if f.endswith('.wav') or f.endswith('.mp3')]\n",
    "\n",
    "w_file = widgets.Select(\n",
    "    options=file_options,\n",
    "    # rows=10,\n",
    "    description='Select a file:',\n",
    "    disabled=False\n",
    ")\n",
    "w_file.layout.width = '100%'\n",
    "\n",
    "w_genre_options = {'B' : 'liquid > dancefloor', 'A': 'dancefloor > liquid'}\n",
    "w_genre = widgets.RadioButtons(\n",
    "    options = w_genre_options.values(),\n",
    "    description='Convert to:',\n",
    "    disabled=False\n",
    ")\n",
    "w_genre.layout.width = '100%'\n",
    "\n",
    "w_button = widgets.Button(\n",
    "    description='Go!',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "w_debug = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Show intermediate steps',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "w_before = widgets.IntSlider(\n",
    "    value=0, min=-4, max=16,\n",
    "    description='Number of downbeats before drop:',\n",
    ")\n",
    "\n",
    "w_length = widgets.IntSlider(\n",
    "    value=2, min=1, max=8,\n",
    "    description='Number of downbeats to process:',\n",
    ")\n",
    "\n",
    "w_out = widgets.Output()\n",
    "\n",
    "\n",
    "widgets_ = {\n",
    "    'file' : w_file, \n",
    "    'genre' : w_genre, \n",
    "    'show_intermediate' : w_debug,\n",
    "    'before' : w_before,\n",
    "    'length' : w_length,\n",
    "}\n",
    "y_orig, y_lws_all, y_no_lws_all, y_orig_all = None, None, None, None\n",
    "def on_button_clicked(b):\n",
    "    global y_orig, y_lws_all, y_no_lws_all, y_orig_all\n",
    "    \n",
    "    with w_out:\n",
    "        IPython.display.clear_output()\n",
    "        \n",
    "        extracts = extract_segments_from_song(\n",
    "            os.path.join('./_music', widgets_['file'].value), \n",
    "            -widgets_['before'].value,\n",
    "            widgets_['length'].value,\n",
    "        )\n",
    "        extracts = list(zip(extracts, [44100] * len(extracts)))\n",
    "        \n",
    "        y_all = [y for y, _ in extracts]\n",
    "        y_lws_all = []\n",
    "        y_no_lws_all = []\n",
    "        y_orig_all = []\n",
    "        \n",
    "        for i, (y, sr) in enumerate(extracts):\n",
    "            print('\\tProcessing extract {:1d}/{:1d}'.format(i+1, len(extracts)))            \n",
    "            # Feed the audio into the model\n",
    "            img_orig = convert_extract_to_png(y, sr, widgets_)\n",
    "            visuals = apply_model()\n",
    "            # Retrieve the generated image, convert to audio, and output\n",
    "            y_lws, y_no_lws, y_lws_orig = display_results(y, img_orig, visuals, widgets_)\n",
    "            # Concatenate into one big audio stream\n",
    "            y_lws_all.append(y_lws)\n",
    "            y_no_lws_all.append(y_no_lws)\n",
    "            y_orig_all.append(y_lws_orig)\n",
    "            \n",
    "        def concat_and_display(y_array, title):\n",
    "            y_ = np.concatenate(y_array)\n",
    "            a_ = IPython.display.Audio(y_, rate=44100,)\n",
    "            print(title)\n",
    "            display(a_)\n",
    "        \n",
    "        print('AMPLITUDE, PHASE:')\n",
    "        concat_and_display(y_all, 'Original')\n",
    "        concat_and_display(y_no_lws_all, 'Transformed, original')\n",
    "        \n",
    "        # Display original audio, that has been first transformed to the Mel scale first,\n",
    "        # then reconstructed from that representation, with phase inferred using RTISI-LA\n",
    "        #\n",
    "        # concat_and_display(y_orig_all, 'Original, mel-scale and then RTISI-LA')\n",
    "        #\n",
    "        # Display transformed audio, where phase has been inferred using RTISI-LA:\n",
    "        #\n",
    "        # concat_and_display(y_lws_all, 'Transformed, RTISI-LA')\n",
    "        \n",
    "w_button.on_click(on_button_clicked)\n",
    "\n",
    "display(w_file)\n",
    "display(w_genre)\n",
    "display(w_before, w_length)\n",
    "display(w_debug)\n",
    "display(w_button)\n",
    "display(w_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all folks :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
